#summary Should we recompute next time, or cache the result until then?

= The Problem =

Enough uses [DifferentialComputing]. This means that every result is actually the application of a difference (delta) to a previous result. This also works in the opposite direction - a previous result can be recomputed by applying the reverse difference on the newer result.

As described in [MemoryManagement], once we compute a result we have two options:

  # Store the result until we need it next time, and possibly discard the old results from which it was derived.
  # Discard the result, and save instead a cheaper (in terms of resources) older result that will be used as the basis for applying differences. When we need our result again, we will recompute it from the old, saved result.

= Formulation as a graph =
This problem can be formulated in the following manner.

A *node* in the graph is a computed result. Each node has its associated "cost" in resources of storing that result. The actual cost of "being" at a node is the amount of time spent at that node, times the cost associated with that node.

An *edge* in the graph is a difference. Each edge has a time-cost associated with it, which represents the amount of time it takes to apply that difference. Thus, to move from node A to node B on an edge E implies wasting the amount of time that is associated with E. An edge can be traveresed in both directions (because deltas are reversible).

The *graph* is the history of an object. Each independant object thus has its own seperate graph. The graph will probably really be acyclic.

